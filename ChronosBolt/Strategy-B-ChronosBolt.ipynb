{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d80c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d87ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Retrieval-Augmented-Time-Series-Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e5fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use only GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f973f32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>S199</th>\n",
       "      <th>S200</th>\n",
       "      <th>S332B</th>\n",
       "      <th>S332BN</th>\n",
       "      <th>S332C</th>\n",
       "      <th>S332D</th>\n",
       "      <th>S332DX1</th>\n",
       "      <th>S12A</th>\n",
       "      <th>S12B</th>\n",
       "      <th>...</th>\n",
       "      <th>S12B_T</th>\n",
       "      <th>S12C_T</th>\n",
       "      <th>S12D_T</th>\n",
       "      <th>S18C</th>\n",
       "      <th>NP205_rain</th>\n",
       "      <th>NP205_stage</th>\n",
       "      <th>NP205_PET</th>\n",
       "      <th>P33_rain</th>\n",
       "      <th>P33_stage</th>\n",
       "      <th>P33_PET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>242.17</td>\n",
       "      <td>279.69</td>\n",
       "      <td>218.42</td>\n",
       "      <td>218.42</td>\n",
       "      <td>445.77</td>\n",
       "      <td>481.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>460</td>\n",
       "      <td>382.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.03</td>\n",
       "      <td>10.06</td>\n",
       "      <td>10.14</td>\n",
       "      <td>289.0</td>\n",
       "      <td>2.28</td>\n",
       "      <td>6.925</td>\n",
       "      <td>2.090</td>\n",
       "      <td>0.106</td>\n",
       "      <td>7.133</td>\n",
       "      <td>2.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>296.05</td>\n",
       "      <td>294.85</td>\n",
       "      <td>265.13</td>\n",
       "      <td>265.13</td>\n",
       "      <td>587.35</td>\n",
       "      <td>556.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490</td>\n",
       "      <td>405.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.17</td>\n",
       "      <td>10.17</td>\n",
       "      <td>10.18</td>\n",
       "      <td>274.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7.025</td>\n",
       "      <td>3.870</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.133</td>\n",
       "      <td>3.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-18</td>\n",
       "      <td>277.85</td>\n",
       "      <td>294.24</td>\n",
       "      <td>264.23</td>\n",
       "      <td>264.23</td>\n",
       "      <td>514.45</td>\n",
       "      <td>503.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>472</td>\n",
       "      <td>394.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.13</td>\n",
       "      <td>10.13</td>\n",
       "      <td>10.15</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.008</td>\n",
       "      <td>3.510</td>\n",
       "      <td>0.088</td>\n",
       "      <td>7.138</td>\n",
       "      <td>3.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-19</td>\n",
       "      <td>224.13</td>\n",
       "      <td>294.72</td>\n",
       "      <td>264.39</td>\n",
       "      <td>264.39</td>\n",
       "      <td>491.06</td>\n",
       "      <td>537.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>466</td>\n",
       "      <td>392.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.12</td>\n",
       "      <td>10.12</td>\n",
       "      <td>10.14</td>\n",
       "      <td>227.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>6.986</td>\n",
       "      <td>1.980</td>\n",
       "      <td>0.256</td>\n",
       "      <td>7.167</td>\n",
       "      <td>1.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-10-20</td>\n",
       "      <td>260.51</td>\n",
       "      <td>295.06</td>\n",
       "      <td>263.38</td>\n",
       "      <td>263.38</td>\n",
       "      <td>489.25</td>\n",
       "      <td>558.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>472</td>\n",
       "      <td>399.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.13</td>\n",
       "      <td>10.14</td>\n",
       "      <td>10.17</td>\n",
       "      <td>276.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>6.977</td>\n",
       "      <td>1.270</td>\n",
       "      <td>0.678</td>\n",
       "      <td>7.195</td>\n",
       "      <td>1.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>2024-12-27</td>\n",
       "      <td>76.07</td>\n",
       "      <td>149.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>317.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.77</td>\n",
       "      <td>9.56</td>\n",
       "      <td>9.54</td>\n",
       "      <td>-28.4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.642</td>\n",
       "      <td>1.270</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.239</td>\n",
       "      <td>1.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>2024-12-28</td>\n",
       "      <td>76.09</td>\n",
       "      <td>149.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>316.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.76</td>\n",
       "      <td>9.54</td>\n",
       "      <td>9.53</td>\n",
       "      <td>60.6</td>\n",
       "      <td>0.02</td>\n",
       "      <td>6.639</td>\n",
       "      <td>1.524</td>\n",
       "      <td>0.100</td>\n",
       "      <td>7.235</td>\n",
       "      <td>1.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>2024-12-29</td>\n",
       "      <td>228.31</td>\n",
       "      <td>97.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>474.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>9.63</td>\n",
       "      <td>9.61</td>\n",
       "      <td>269.0</td>\n",
       "      <td>1.84</td>\n",
       "      <td>6.755</td>\n",
       "      <td>1.016</td>\n",
       "      <td>1.750</td>\n",
       "      <td>7.294</td>\n",
       "      <td>1.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>188.28</td>\n",
       "      <td>149.24</td>\n",
       "      <td>9.90</td>\n",
       "      <td>9.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>535.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.84</td>\n",
       "      <td>9.62</td>\n",
       "      <td>9.60</td>\n",
       "      <td>302.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.800</td>\n",
       "      <td>2.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.327</td>\n",
       "      <td>2.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>206.83</td>\n",
       "      <td>188.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>352.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.82</td>\n",
       "      <td>9.60</td>\n",
       "      <td>9.59</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.795</td>\n",
       "      <td>2.794</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.321</td>\n",
       "      <td>2.794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1538 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date    S199    S200   S332B  S332BN   S332C   S332D  S332DX1  \\\n",
       "0     2020-10-16  242.17  279.69  218.42  218.42  445.77  481.38      0.0   \n",
       "1     2020-10-17  296.05  294.85  265.13  265.13  587.35  556.24      0.0   \n",
       "2     2020-10-18  277.85  294.24  264.23  264.23  514.45  503.14      0.0   \n",
       "3     2020-10-19  224.13  294.72  264.39  264.39  491.06  537.91      0.0   \n",
       "4     2020-10-20  260.51  295.06  263.38  263.38  489.25  558.77      0.0   \n",
       "...          ...     ...     ...     ...     ...     ...     ...      ...   \n",
       "1533  2024-12-27   76.07  149.85    0.00    0.00    0.00  317.51      0.0   \n",
       "1534  2024-12-28   76.09  149.96    0.00    0.00    0.00  316.96      0.0   \n",
       "1535  2024-12-29  228.31   97.82    0.00    0.00    0.00  474.71      0.0   \n",
       "1536  2024-12-30  188.28  149.24    9.90    9.90    0.00  535.99      0.0   \n",
       "1537  2024-12-31  206.83  188.69    0.00    0.00    0.00  352.78      0.0   \n",
       "\n",
       "      S12A   S12B  ...  S12B_T  S12C_T  S12D_T   S18C  NP205_rain  \\\n",
       "0      460  382.0  ...   10.03   10.06   10.14  289.0        2.28   \n",
       "1      490  405.0  ...   10.17   10.17   10.18  274.0        0.01   \n",
       "2      472  394.0  ...   10.13   10.13   10.15  252.0        0.00   \n",
       "3      466  392.0  ...   10.12   10.12   10.14  227.0        0.11   \n",
       "4      472  399.0  ...   10.13   10.14   10.17  276.0        0.37   \n",
       "...    ...    ...  ...     ...     ...     ...    ...         ...   \n",
       "1533     0    0.0  ...    8.77    9.56    9.54  -28.4        0.01   \n",
       "1534     0    0.0  ...    8.76    9.54    9.53   60.6        0.02   \n",
       "1535     0    0.0  ...    8.85    9.63    9.61  269.0        1.84   \n",
       "1536     0    0.0  ...    8.84    9.62    9.60  302.0        0.00   \n",
       "1537     0    0.0  ...    8.82    9.60    9.59  156.0        0.00   \n",
       "\n",
       "      NP205_stage  NP205_PET  P33_rain  P33_stage  P33_PET  \n",
       "0           6.925      2.090     0.106      7.133    2.090  \n",
       "1           7.025      3.870     0.000      7.133    3.870  \n",
       "2           7.008      3.510     0.088      7.138    3.510  \n",
       "3           6.986      1.980     0.256      7.167    1.980  \n",
       "4           6.977      1.270     0.678      7.195    1.270  \n",
       "...           ...        ...       ...        ...      ...  \n",
       "1533        6.642      1.270     0.000      7.239    1.270  \n",
       "1534        6.639      1.524     0.100      7.235    1.524  \n",
       "1535        6.755      1.016     1.750      7.294    1.016  \n",
       "1536        6.800      2.032     0.000      7.327    2.032  \n",
       "1537        6.795      2.794     0.000      7.321    2.794  \n",
       "\n",
       "[1538 rows x 39 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= pd.read_csv(\"filepath\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913710f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1538 entries, 0 to 1537\n",
      "Data columns (total 39 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   date              1538 non-null   datetime64[ns]\n",
      " 1   S199              1538 non-null   float64       \n",
      " 2   S200              1538 non-null   float64       \n",
      " 3   S332B             1538 non-null   float64       \n",
      " 4   S332BN            1538 non-null   float64       \n",
      " 5   S332C             1538 non-null   float64       \n",
      " 6   S332D             1538 non-null   float64       \n",
      " 7   S332DX1           1538 non-null   float64       \n",
      " 8   S12A              1538 non-null   int64         \n",
      " 9   S12B              1538 non-null   float64       \n",
      " 10  S12C              1538 non-null   int64         \n",
      " 11  S12D              1538 non-null   int64         \n",
      " 12  S333              1538 non-null   float64       \n",
      " 13  S333N             1538 non-null   float64       \n",
      " 14  S334              1538 non-null   float64       \n",
      " 15  S343A             1538 non-null   float64       \n",
      " 16  S343B             1538 non-null   float64       \n",
      " 17  S344              1538 non-null   float64       \n",
      " 18  S356              1538 non-null   float64       \n",
      " 19  S333_T            1538 non-null   float64       \n",
      " 20  S343B_T           1538 non-null   float64       \n",
      " 21  S334_H            1538 non-null   float64       \n",
      " 22  SWEVER4_stage     1538 non-null   float64       \n",
      " 23  TSH_stage         1538 non-null   float64       \n",
      " 24  NP62_stage        1538 non-null   float64       \n",
      " 25  G620_water_level  1538 non-null   float64       \n",
      " 26  NESRS1            1538 non-null   float64       \n",
      " 27  NESRS2            1538 non-null   float64       \n",
      " 28  S12A_T            1538 non-null   float64       \n",
      " 29  S12B_T            1538 non-null   float64       \n",
      " 30  S12C_T            1538 non-null   float64       \n",
      " 31  S12D_T            1538 non-null   float64       \n",
      " 32  S18C              1538 non-null   float64       \n",
      " 33  NP205_rain        1538 non-null   float64       \n",
      " 34  NP205_stage       1538 non-null   float64       \n",
      " 35  NP205_PET         1538 non-null   float64       \n",
      " 36  P33_rain          1538 non-null   float64       \n",
      " 37  P33_stage         1538 non-null   float64       \n",
      " 38  P33_PET           1538 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(35), int64(3)\n",
      "memory usage: 468.7 KB\n",
      "None\n",
      "           date formatted_date\n",
      "0    2020-10-16     10/16/2020\n",
      "1    2020-10-17     10/17/2020\n",
      "2    2020-10-18     10/18/2020\n",
      "3    2020-10-19     10/19/2020\n",
      "4    2020-10-20     10/20/2020\n",
      "...         ...            ...\n",
      "1533 2024-12-27     12/27/2024\n",
      "1534 2024-12-28     12/28/2024\n",
      "1535 2024-12-29     12/29/2024\n",
      "1536 2024-12-30     12/30/2024\n",
      "1537 2024-12-31     12/31/2024\n",
      "\n",
      "[1538 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>S199</th>\n",
       "      <th>S200</th>\n",
       "      <th>S332B</th>\n",
       "      <th>S332BN</th>\n",
       "      <th>S332C</th>\n",
       "      <th>S332D</th>\n",
       "      <th>S332DX1</th>\n",
       "      <th>S12A</th>\n",
       "      <th>S12B</th>\n",
       "      <th>...</th>\n",
       "      <th>S12B_T</th>\n",
       "      <th>S12C_T</th>\n",
       "      <th>S12D_T</th>\n",
       "      <th>S18C</th>\n",
       "      <th>NP205_rain</th>\n",
       "      <th>NP205_stage</th>\n",
       "      <th>NP205_PET</th>\n",
       "      <th>P33_rain</th>\n",
       "      <th>P33_stage</th>\n",
       "      <th>P33_PET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10/16/2020</td>\n",
       "      <td>242.17</td>\n",
       "      <td>279.69</td>\n",
       "      <td>218.42</td>\n",
       "      <td>218.42</td>\n",
       "      <td>445.77</td>\n",
       "      <td>481.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>460</td>\n",
       "      <td>382.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.03</td>\n",
       "      <td>10.06</td>\n",
       "      <td>10.14</td>\n",
       "      <td>289.0</td>\n",
       "      <td>2.28</td>\n",
       "      <td>6.925</td>\n",
       "      <td>2.090</td>\n",
       "      <td>0.106</td>\n",
       "      <td>7.133</td>\n",
       "      <td>2.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/17/2020</td>\n",
       "      <td>296.05</td>\n",
       "      <td>294.85</td>\n",
       "      <td>265.13</td>\n",
       "      <td>265.13</td>\n",
       "      <td>587.35</td>\n",
       "      <td>556.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490</td>\n",
       "      <td>405.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.17</td>\n",
       "      <td>10.17</td>\n",
       "      <td>10.18</td>\n",
       "      <td>274.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7.025</td>\n",
       "      <td>3.870</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.133</td>\n",
       "      <td>3.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/18/2020</td>\n",
       "      <td>277.85</td>\n",
       "      <td>294.24</td>\n",
       "      <td>264.23</td>\n",
       "      <td>264.23</td>\n",
       "      <td>514.45</td>\n",
       "      <td>503.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>472</td>\n",
       "      <td>394.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.13</td>\n",
       "      <td>10.13</td>\n",
       "      <td>10.15</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.008</td>\n",
       "      <td>3.510</td>\n",
       "      <td>0.088</td>\n",
       "      <td>7.138</td>\n",
       "      <td>3.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10/19/2020</td>\n",
       "      <td>224.13</td>\n",
       "      <td>294.72</td>\n",
       "      <td>264.39</td>\n",
       "      <td>264.39</td>\n",
       "      <td>491.06</td>\n",
       "      <td>537.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>466</td>\n",
       "      <td>392.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.12</td>\n",
       "      <td>10.12</td>\n",
       "      <td>10.14</td>\n",
       "      <td>227.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>6.986</td>\n",
       "      <td>1.980</td>\n",
       "      <td>0.256</td>\n",
       "      <td>7.167</td>\n",
       "      <td>1.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10/20/2020</td>\n",
       "      <td>260.51</td>\n",
       "      <td>295.06</td>\n",
       "      <td>263.38</td>\n",
       "      <td>263.38</td>\n",
       "      <td>489.25</td>\n",
       "      <td>558.77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>472</td>\n",
       "      <td>399.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.13</td>\n",
       "      <td>10.14</td>\n",
       "      <td>10.17</td>\n",
       "      <td>276.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>6.977</td>\n",
       "      <td>1.270</td>\n",
       "      <td>0.678</td>\n",
       "      <td>7.195</td>\n",
       "      <td>1.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>12/27/2024</td>\n",
       "      <td>76.07</td>\n",
       "      <td>149.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>317.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.77</td>\n",
       "      <td>9.56</td>\n",
       "      <td>9.54</td>\n",
       "      <td>-28.4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.642</td>\n",
       "      <td>1.270</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.239</td>\n",
       "      <td>1.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>12/28/2024</td>\n",
       "      <td>76.09</td>\n",
       "      <td>149.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>316.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.76</td>\n",
       "      <td>9.54</td>\n",
       "      <td>9.53</td>\n",
       "      <td>60.6</td>\n",
       "      <td>0.02</td>\n",
       "      <td>6.639</td>\n",
       "      <td>1.524</td>\n",
       "      <td>0.100</td>\n",
       "      <td>7.235</td>\n",
       "      <td>1.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>12/29/2024</td>\n",
       "      <td>228.31</td>\n",
       "      <td>97.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>474.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.85</td>\n",
       "      <td>9.63</td>\n",
       "      <td>9.61</td>\n",
       "      <td>269.0</td>\n",
       "      <td>1.84</td>\n",
       "      <td>6.755</td>\n",
       "      <td>1.016</td>\n",
       "      <td>1.750</td>\n",
       "      <td>7.294</td>\n",
       "      <td>1.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>12/30/2024</td>\n",
       "      <td>188.28</td>\n",
       "      <td>149.24</td>\n",
       "      <td>9.90</td>\n",
       "      <td>9.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>535.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.84</td>\n",
       "      <td>9.62</td>\n",
       "      <td>9.60</td>\n",
       "      <td>302.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.800</td>\n",
       "      <td>2.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.327</td>\n",
       "      <td>2.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>12/31/2024</td>\n",
       "      <td>206.83</td>\n",
       "      <td>188.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>352.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.82</td>\n",
       "      <td>9.60</td>\n",
       "      <td>9.59</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.795</td>\n",
       "      <td>2.794</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.321</td>\n",
       "      <td>2.794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1538 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date    S199    S200   S332B  S332BN   S332C   S332D  S332DX1  \\\n",
       "0     10/16/2020  242.17  279.69  218.42  218.42  445.77  481.38      0.0   \n",
       "1     10/17/2020  296.05  294.85  265.13  265.13  587.35  556.24      0.0   \n",
       "2     10/18/2020  277.85  294.24  264.23  264.23  514.45  503.14      0.0   \n",
       "3     10/19/2020  224.13  294.72  264.39  264.39  491.06  537.91      0.0   \n",
       "4     10/20/2020  260.51  295.06  263.38  263.38  489.25  558.77      0.0   \n",
       "...          ...     ...     ...     ...     ...     ...     ...      ...   \n",
       "1533  12/27/2024   76.07  149.85    0.00    0.00    0.00  317.51      0.0   \n",
       "1534  12/28/2024   76.09  149.96    0.00    0.00    0.00  316.96      0.0   \n",
       "1535  12/29/2024  228.31   97.82    0.00    0.00    0.00  474.71      0.0   \n",
       "1536  12/30/2024  188.28  149.24    9.90    9.90    0.00  535.99      0.0   \n",
       "1537  12/31/2024  206.83  188.69    0.00    0.00    0.00  352.78      0.0   \n",
       "\n",
       "      S12A   S12B  ...  S12B_T  S12C_T  S12D_T   S18C  NP205_rain  \\\n",
       "0      460  382.0  ...   10.03   10.06   10.14  289.0        2.28   \n",
       "1      490  405.0  ...   10.17   10.17   10.18  274.0        0.01   \n",
       "2      472  394.0  ...   10.13   10.13   10.15  252.0        0.00   \n",
       "3      466  392.0  ...   10.12   10.12   10.14  227.0        0.11   \n",
       "4      472  399.0  ...   10.13   10.14   10.17  276.0        0.37   \n",
       "...    ...    ...  ...     ...     ...     ...    ...         ...   \n",
       "1533     0    0.0  ...    8.77    9.56    9.54  -28.4        0.01   \n",
       "1534     0    0.0  ...    8.76    9.54    9.53   60.6        0.02   \n",
       "1535     0    0.0  ...    8.85    9.63    9.61  269.0        1.84   \n",
       "1536     0    0.0  ...    8.84    9.62    9.60  302.0        0.00   \n",
       "1537     0    0.0  ...    8.82    9.60    9.59  156.0        0.00   \n",
       "\n",
       "      NP205_stage  NP205_PET  P33_rain  P33_stage  P33_PET  \n",
       "0           6.925      2.090     0.106      7.133    2.090  \n",
       "1           7.025      3.870     0.000      7.133    3.870  \n",
       "2           7.008      3.510     0.088      7.138    3.510  \n",
       "3           6.986      1.980     0.256      7.167    1.980  \n",
       "4           6.977      1.270     0.678      7.195    1.270  \n",
       "...           ...        ...       ...        ...      ...  \n",
       "1533        6.642      1.270     0.000      7.239    1.270  \n",
       "1534        6.639      1.524     0.100      7.235    1.524  \n",
       "1535        6.755      1.016     1.750      7.294    1.016  \n",
       "1536        6.800      2.032     0.000      7.327    2.032  \n",
       "1537        6.795      2.794     0.000      7.321    2.794  \n",
       "\n",
       "[1538 rows x 39 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'date' column to proper datetime format\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "print(data.info())\n",
    "\n",
    "data['formatted_date'] = data['date'].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "print(data[['date', 'formatted_date']])                                                                                                    # Drop the 'formatted_date' column if not needed\n",
    "data = data.drop(columns=['formatted_date'])\n",
    "\n",
    "data['date'] = data['date'].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0615b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      unique_id         ds        y\n",
      "0          S199 2020-10-16  242.170\n",
      "1          S199 2020-10-17  296.050\n",
      "2          S199 2020-10-18  277.850\n",
      "3          S199 2020-10-19  224.130\n",
      "4          S199 2020-10-20  260.510\n",
      "...         ...        ...      ...\n",
      "58439   P33_PET 2024-12-27    1.270\n",
      "58440   P33_PET 2024-12-28    1.524\n",
      "58441   P33_PET 2024-12-29    1.016\n",
      "58442   P33_PET 2024-12-30    2.032\n",
      "58443   P33_PET 2024-12-31    2.794\n",
      "\n",
      "[58444 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S199</td>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>242.170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S199</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>296.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S199</td>\n",
       "      <td>2020-10-18</td>\n",
       "      <td>277.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S199</td>\n",
       "      <td>2020-10-19</td>\n",
       "      <td>224.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S199</td>\n",
       "      <td>2020-10-20</td>\n",
       "      <td>260.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58439</th>\n",
       "      <td>P33_PET</td>\n",
       "      <td>2024-12-27</td>\n",
       "      <td>1.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58440</th>\n",
       "      <td>P33_PET</td>\n",
       "      <td>2024-12-28</td>\n",
       "      <td>1.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58441</th>\n",
       "      <td>P33_PET</td>\n",
       "      <td>2024-12-29</td>\n",
       "      <td>1.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58442</th>\n",
       "      <td>P33_PET</td>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>2.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58443</th>\n",
       "      <td>P33_PET</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>2.794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58444 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      unique_id         ds   target\n",
       "0          S199 2020-10-16  242.170\n",
       "1          S199 2020-10-17  296.050\n",
       "2          S199 2020-10-18  277.850\n",
       "3          S199 2020-10-19  224.130\n",
       "4          S199 2020-10-20  260.510\n",
       "...         ...        ...      ...\n",
       "58439   P33_PET 2024-12-27    1.270\n",
       "58440   P33_PET 2024-12-28    1.524\n",
       "58441   P33_PET 2024-12-29    1.016\n",
       "58442   P33_PET 2024-12-30    2.032\n",
       "58443   P33_PET 2024-12-31    2.794\n",
       "\n",
       "[58444 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace with your actual DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#Convert 'date' to datetime format if not \n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "#Melt the DataFrame to long format\n",
    "long_df = pd.melt(df, id_vars=['date'], \n",
    "                  value_vars=[col for col in df.columns if col != 'date'], \n",
    "                  var_name='unique_id', \n",
    "                  value_name='y')\n",
    "\n",
    "#Rename the date column to 'ds'\n",
    "long_df.rename(columns={'date': 'ds'}, inplace=True)              \n",
    "\n",
    "# Reorder columns\n",
    "long_df = long_df[['unique_id', 'ds', 'y']]\n",
    "\n",
    "long_df\n",
    "\n",
    "# Display the resulting long format DataFrame\n",
    "print(long_df) \n",
    "\n",
    "Y_df = long_df[['unique_id', 'ds', 'y']]                                                                                                                                                     # Rename the 'y' column to 'target'\n",
    "Y_df = Y_df.rename(columns={'y': 'target'})\n",
    "Y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1a2b933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 38 series.\n"
     ]
    }
   ],
   "source": [
    "# 2. Build per-series arrays\n",
    "series_map = {}\n",
    "dates_map  = {}\n",
    "\n",
    "for uid, grp in Y_df.groupby('unique_id'):\n",
    "    vals = grp['target'].to_numpy(dtype=np.float32)\n",
    "    ds_list = grp['ds'].tolist()\n",
    "    series_map[uid] = vals\n",
    "    dates_map[uid]  = ds_list\n",
    "\n",
    "print(f\"Prepared {len(series_map)} series.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6760d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K=4 7 days SIMRAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Logging\n",
    "# ----------------------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "logger = logging.getLogger(\"RetrievalAug\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# ----------------------------------------------------------------------\n",
    "prediction_length = 7\n",
    "input  = 100                     #History length\n",
    "top_n  = 4                       # 0 = pure baseline \n",
    "aggregate_mode = \"mean\"          \n",
    "chronos_model_id = \"amazon/chronos-bolt-small\"\n",
    "\n",
    "eval_start = pd.Timestamp(\"2024-05-15\")\n",
    "eval_end   = pd.Timestamp(\"2024-12-31\")\n",
    "eval_uids  = [\"NP205_stage\",\"P33_stage\",\"G620_water_level\",\"NESRS1\",\"NESRS2\"]\n",
    "\n",
    "min_history_points = input\n",
    "\n",
    "# overlap filter\n",
    "OVERLAP_THRESH = 0.70            # >= 70% overlap -> drop duplicate window from same uid\n",
    "\n",
    "H = input\n",
    "P = prediction_length\n",
    "AUG_LEN = 2 * H + P              # used only for retrieval variants\n",
    "\n",
    "print(f\"[CONFIG] H={H}, P={P}, top_n={top_n}, aggregate={aggregate_mode}, \"\n",
    "      f\"AUG_LEN={AUG_LEN}, overlap_thr={OVERLAP_THRESH}\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# REQUIRED EXTERNAL OBJECTS (series_map, dates_map) must exist\n",
    "# ----------------------------------------------------------------------\n",
    "from chronos_bolt_adapter import ChronosBoltAdapter\n",
    "\n",
    "device_map = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = (torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported())\n",
    "               else torch.float32)\n",
    "\n",
    "chronos = ChronosBoltAdapter(\n",
    "    model_id=chronos_model_id,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch_dtype,\n",
    "    faux_num_samples=20\n",
    ")\n",
    "\n",
    "eval_uids = [u for u in eval_uids if u in series_map]\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Build date index map\n",
    "# ----------------------------------------------------------------------\n",
    "def build_date_index(dates_map):\n",
    "    out = {}\n",
    "    for uid, dl in dates_map.items():\n",
    "        dlist = [pd.Timestamp(d) for d in dl]\n",
    "        if not pd.Index(dlist).is_monotonic_increasing:\n",
    "            order = np.argsort(dlist)\n",
    "            dlist = [dlist[i] for i in order]\n",
    "        idx_map = {d: i for i, d in enumerate(dlist)}\n",
    "        out[uid] = (dlist, idx_map)\n",
    "    return out\n",
    "\n",
    "DATE_INDEX = build_date_index(dates_map)\n",
    "\n",
    "def date_to_cutoff(uid, ds):\n",
    "    ds = pd.Timestamp(ds)\n",
    "    if uid not in DATE_INDEX:\n",
    "        return None\n",
    "    _, im = DATE_INDEX[uid]\n",
    "    return im.get(ds, None)\n",
    "\n",
    "station_mean_std = {\n",
    "    k: (float(np.mean(v)), float(np.std(v) + 1e-7))\n",
    "    for k, v in series_map.items()\n",
    "}\n",
    "\n",
    "def denormalize_predictions(pred, mean_std):\n",
    "    mean, std = mean_std\n",
    "    return pred * std + mean\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# BUILD CANDIDATE BANK  (H history + P future)\n",
    "# ----------------------------------------------------------------------\n",
    "cand_full_segments = []\n",
    "cand_series        = []\n",
    "cand_end_idx       = []\n",
    "cand_start_idx     = []\n",
    "\n",
    "t_build = time.time()\n",
    "for uid, values in series_map.items():\n",
    "    x = np.asarray(values, dtype=float)\n",
    "    if len(x) < H + P:\n",
    "        continue\n",
    "\n",
    "    dates_uid = [pd.Timestamp(d) for d in dates_map[uid]]\n",
    "    stride = 1\n",
    "    for end in range(H, len(x) - P + 1, stride):\n",
    "        end_date = dates_uid[end - 1]\n",
    "        if end_date >= eval_start:  \n",
    "            continue\n",
    "\n",
    "        start = end - H\n",
    "        hist_seg = x[start:end]\n",
    "        fut_seg  = x[end:end+P]\n",
    "\n",
    "        if np.isnan(hist_seg).all() or np.isnan(fut_seg).all():\n",
    "            continue\n",
    "\n",
    "        hist_seg = pd.Series(hist_seg).ffill().bfill().values\n",
    "        fut_seg  = pd.Series(fut_seg).ffill().bfill().values\n",
    "\n",
    "        full_seg = np.concatenate([hist_seg, fut_seg]) \n",
    "        cand_full_segments.append(full_seg.astype(np.float32))\n",
    "        cand_series.append(uid)\n",
    "        cand_end_idx.append(end)\n",
    "        cand_start_idx.append(start)\n",
    "\n",
    "cand_full_segments = (np.stack(cand_full_segments)\n",
    "                      if cand_full_segments else np.zeros((0, H+P), dtype=np.float32))\n",
    "cand_series    = np.array(cand_series)\n",
    "cand_end_idx   = np.array(cand_end_idx)\n",
    "cand_start_idx = np.array(cand_start_idx)\n",
    "\n",
    "print(f\"[BUILD] Candidate segments (history+future): {cand_full_segments.shape} \"\n",
    "      f\"built in {time.time() - t_build:.2f}s\")\n",
    "\n",
    "uid_to_cand_indices = {}\n",
    "for i, s in enumerate(cand_series):\n",
    "    uid_to_cand_indices.setdefault(s, []).append(i)\n",
    "\n",
    "cand_tensor_full = torch.from_numpy(cand_full_segments)     \n",
    "cand_hist_tensor = cand_tensor_full[:, :H]                \n",
    "cand_hist_mean   = cand_hist_tensor.mean(dim=1)\n",
    "cand_hist_std    = cand_hist_tensor.std(dim=1) + 1e-6\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Overlap helper\n",
    "# ----------------------------------------------------------------------\n",
    "def window_overlap_ratio(a_start, a_end, b_start, b_end):\n",
    "    inter = max(0, min(a_end, b_end) - max(a_start, b_start))\n",
    "    length = max(a_end - a_start, 1)\n",
    "    return inter / length\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# RETRIEVAL (Euclidean)\n",
    "# ----------------------------------------------------------------------\n",
    "def retrieve_topn(query_series, query_uid, cutoff_idx, top_n):\n",
    "    if top_n == 0:\n",
    "        return np.array([], dtype=int), np.array([], dtype=float), query_series[cutoff_idx - H: cutoff_idx]\n",
    "\n",
    "    if cutoff_idx is None or cutoff_idx < H or cutoff_idx + P > len(query_series):\n",
    "        return None\n",
    "\n",
    "    ctx = query_series[cutoff_idx - H: cutoff_idx]\n",
    "    ctx = pd.Series(ctx).ffill().bfill().values\n",
    "\n",
    "    q = torch.from_numpy(ctx.astype(np.float32)).unsqueeze(0)  \n",
    "    with torch.no_grad():\n",
    "        dists = torch.cdist(q, cand_hist_tensor, p=2).squeeze(0)\n",
    "\n",
    "    for idx in uid_to_cand_indices.get(query_uid, []):\n",
    "        if cand_end_idx[idx] >= (cutoff_idx - 1):\n",
    "            dists[idx] = float(\"inf\")\n",
    "\n",
    "    valid = (~torch.isinf(dists)).nonzero(as_tuple=True)[0]\n",
    "    if valid.numel() == 0:\n",
    "        return None\n",
    "\n",
    "    d_valid = dists[valid]\n",
    "    order = torch.argsort(d_valid, descending=False)\n",
    "\n",
    "    chosen, d_sel = [], []\n",
    "    for pos in order:\n",
    "        if len(chosen) >= top_n:\n",
    "            break\n",
    "        ci = int(valid[pos])\n",
    "        keep = True\n",
    "        for t in chosen:\n",
    "            if cand_series[ci] == cand_series[t]:\n",
    "                ov = window_overlap_ratio(\n",
    "                    cand_start_idx[ci], cand_end_idx[ci],\n",
    "                    cand_start_idx[t],   cand_end_idx[t]\n",
    "                )\n",
    "                if ov >= OVERLAP_THRESH:\n",
    "                    keep = False\n",
    "                    break\n",
    "        if keep:\n",
    "            chosen.append(ci)\n",
    "            d_sel.append(float(d_valid[pos]))\n",
    "\n",
    "    return np.array(chosen, dtype=int), np.array(d_sel, dtype=float), ctx\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# VARIANT BUILDERS\n",
    "# ----------------------------------------------------------------------\n",
    "def build_baseline_variant(query_ctx):\n",
    "    t = torch.tensor(query_ctx, dtype=torch.float32)\n",
    "    m = float(t.mean())\n",
    "    s = float(t.std(unbiased=False) + 1e-7)\n",
    "    ctx_n = (t - m) / s\n",
    "    return [ctx_n], [(m, s)]\n",
    "\n",
    "def build_augmented_variants(query_ctx, top_indices):\n",
    "    ctx_t = torch.tensor(query_ctx, dtype=torch.float32)\n",
    "    m_c = float(ctx_t.mean())\n",
    "    s_c = float(ctx_t.std(unbiased=False) + 1e-7)\n",
    "    ctx_n = (ctx_t - m_c) / s_c\n",
    "    variants, stats = [], []\n",
    "    zero_prefix = torch.zeros(H + P, dtype=torch.float32)\n",
    "    variants.append(torch.cat([zero_prefix, ctx_n]))\n",
    "    stats.append((m_c, s_c))\n",
    "\n",
    "    for ci in top_indices:\n",
    "        seg_full = cand_full_segments[ci]\n",
    "        seg_t = torch.tensor(seg_full, dtype=torch.float32)\n",
    "        m_s = float(seg_t.mean())\n",
    "        s_s = float(seg_t.std(unbiased=False) + 1e-7)\n",
    "        seg_n = (seg_t - m_s) / s_s\n",
    "        shift = ctx_n[0] - seg_n[-1]\n",
    "        seg_n = seg_n + shift\n",
    "\n",
    "        variants.append(torch.cat([seg_n, ctx_n]))\n",
    "        stats.append((m_c, s_c))\n",
    "\n",
    "    for v in variants:\n",
    "        assert v.shape[0] == AUG_LEN, f\"Variant length mismatch {v.shape[0]} != {AUG_LEN}\"\n",
    "\n",
    "    return variants, stats\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# FORECAST LOOP\n",
    "# ----------------------------------------------------------------------\n",
    "retrieval_audit = []\n",
    "records = []\n",
    "eval_dates = pd.date_range(eval_start, eval_end, freq=\"D\")\n",
    "\n",
    "t_loop = time.time()\n",
    "for ds in tqdm(eval_dates, desc=\"Euclidean Retrieval/Baseline\"):\n",
    "    for uid in eval_uids:\n",
    "        cutoff = date_to_cutoff(uid, ds)\n",
    "        if cutoff is None:\n",
    "            continue\n",
    "\n",
    "        series_vals = np.asarray(series_map[uid], dtype=float)\n",
    "        if cutoff + P > len(series_vals) or cutoff < min_history_points:\n",
    "            continue\n",
    "\n",
    "        # baseline \n",
    "        if top_n == 0:\n",
    "            query_ctx = pd.Series(series_vals[cutoff - H: cutoff]).ffill().bfill().values\n",
    "            variants, stats = build_baseline_variant(query_ctx)\n",
    "            baseline_mode = True\n",
    "            top_idx, top_dists = np.array([],int), np.array([],float)\n",
    "        else:\n",
    "            ret = retrieve_topn(series_vals, uid, cutoff, top_n)\n",
    "            if ret is None:\n",
    "                continue\n",
    "            top_idx, top_dists, query_ctx = ret\n",
    "            variants, stats = build_augmented_variants(query_ctx, top_idx)\n",
    "            baseline_mode = False\n",
    "\n",
    "        # model call\n",
    "        with torch.no_grad():\n",
    "            samples = chronos.predict(variants, prediction_length=P, num_samples=1)\n",
    "\n",
    "        fc_matrix = samples[:, 0, :].cpu().numpy()\n",
    "\n",
    "        fc_matrix_denorm = np.array([\n",
    "            fc_matrix[i] * stats[i][1] + stats[i][0]\n",
    "            for i in range(len(fc_matrix))\n",
    "        ])\n",
    "        if baseline_mode:\n",
    "            final_fc = fc_matrix_denorm[0]\n",
    "        else:\n",
    "            if aggregate_mode == \"mean\":\n",
    "                final_fc = fc_matrix_denorm.mean(axis=0)\n",
    "            else:\n",
    "                anchor = np.median(top_dists) if top_dists.size>0 else 0\n",
    "                d_all = np.concatenate([[anchor], top_dists])\n",
    "                w = np.exp(-alpha * d_all); w /= w.sum()\n",
    "                final_fc = (fc_matrix_denorm * w[:,None]).sum(axis=0)\n",
    "\n",
    "        # Truth + record\n",
    "        truth_future = series_vals[cutoff:cutoff + P]\n",
    "        for h in range(P):\n",
    "            records.append({\n",
    "                \"unique_id\": uid,\n",
    "                \"ds\": ds,\n",
    "                \"horizon\": h+1,\n",
    "                \"pred\": float(final_fc[h]),\n",
    "                \"truth\": float(truth_future[h]),\n",
    "            })\n",
    "\n",
    "print(f\"[LOOP] Completed in {time.time() - t_loop:.2f}s; records: {len(records)}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# RAW PREDICTIONS CSV\n",
    "# ----------------------------------------------------------------------\n",
    "# CSV  write\n",
    "raw_preds_path = Path(f\"KSTUDY_raw_preds_top{top_n}_{prediction_length}_days_Sim.csv\")\n",
    "\n",
    "df_aug = pd.DataFrame(records)\n",
    "if df_aug.empty:\n",
    "    raise RuntimeError(\"No augmented rows. Adjust params / dates / data coverage.\")\n",
    "\n",
    "# write raw preds & truth to a single CSV\n",
    "df_aug.to_csv(raw_preds_path, index=False)\n",
    "logger.info(f\"Wrote raw predictions to {raw_preds_path}\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# METRICS \n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "df_aug = pd.DataFrame(records)\n",
    "if df_aug.empty:\n",
    "    raise RuntimeError(\"No augmented rows. Adjust params / dates / data coverage.\")\n",
    "\n",
    "# overall\n",
    "err_all = df_aug.pred - df_aug.truth\n",
    "overall_mae  = float(np.mean(np.abs(err_all)))\n",
    "overall_rmse = float(np.sqrt(np.mean(err_all**2)))\n",
    "\n",
    "print(\"=== Euclidean Retrieval ===\")\n",
    "\n",
    "# per-station\n",
    "def station_metrics(group):\n",
    "    err = group.pred - group.truth\n",
    "    mae  = float(np.mean(np.abs(err)))\n",
    "    rmse = float(np.sqrt(np.mean(err**2)))\n",
    "    return pd.Series({\"MAE\": mae, \"RMSE\": rmse})\n",
    "\n",
    "per_station = (\n",
    "    df_aug\n",
    "      .groupby(\"unique_id\")\n",
    "      .apply(station_metrics)\n",
    "      .reset_index()\n",
    "      .rename(columns={\"unique_id\": \"station\"})\n",
    ")\n",
    "\n",
    "print(\"Per-station metrics:\")\n",
    "print(per_station.to_string(index=False))\n",
    "\n",
    "# average of station-level MAE/RMSE\n",
    "avg_mae  = per_station.MAE.mean()\n",
    "avg_rmse = per_station.RMSE.mean()\n",
    "\n",
    "print(f\"\\nAVERAGE ACROSS STATIONS: MAE={avg_mae:.6f} | RMSE={avg_rmse:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b207ba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K=4 7 days MIRAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce546e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import logging\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# ======================================================================\n",
    "# Logging\n",
    "# ======================================================================\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "logger = logging.getLogger(\"RetrievalAug_MI\")\n",
    "\n",
    "# ======================================================================\n",
    "# CONFIG\n",
    "# ======================================================================\n",
    "prediction_length = 7\n",
    "H = 100                      # history length \n",
    "top_n = 4                    # number of retrieved neighbors (0 = pure baseline)\n",
    "aggregate_mode = \"mean\"      \n",
    "chronos_model_id = \"amazon/chronos-bolt-small\"\n",
    "\n",
    "eval_start = pd.Timestamp(\"2024-05-15\")\n",
    "eval_end   = pd.Timestamp(\"2024-12-31\")\n",
    "eval_uids  = [\"NP205_stage\",\"P33_stage\",\"G620_water_level\",\"NESRS1\",\"NESRS2\"]\n",
    "min_history_points = H\n",
    "\n",
    "OVERLAP_THRESH = 0.70        # >= 70% overlap => treat as duplicate\n",
    "\n",
    "P = prediction_length\n",
    "AUG_LEN = 2 * H + P\n",
    "print(f\"[CONFIG] H={H}, P={P}, top_n={top_n}, aggregate={aggregate_mode}, \"\n",
    "      f\"AUG_LEN={AUG_LEN} (retrieval mode only), overlap_thr={OVERLAP_THRESH}\")\n",
    "\n",
    "# ======================================================================\n",
    "# REQUIRED EXTERNAL OBJECTS (series_map, dates_map)\n",
    "# ======================================================================\n",
    "from chronos_bolt_adapter import ChronosBoltAdapter  \n",
    "\n",
    "device_map = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = (torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported())\n",
    "               else torch.float32)\n",
    "\n",
    "chronos = ChronosBoltAdapter(\n",
    "    model_id=chronos_model_id,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch_dtype,\n",
    "    faux_num_samples=20  \n",
    ")\n",
    "\n",
    "eval_uids = [u for u in eval_uids if u in series_map]\n",
    "\n",
    "# ======================================================================\n",
    "# Date index map\n",
    "# ======================================================================\n",
    "def build_date_index(dates_map):\n",
    "    out = {}\n",
    "    for uid, dl in dates_map.items():\n",
    "        dlist = [pd.Timestamp(d) for d in dl]\n",
    "        if not pd.Index(dlist).is_monotonic_increasing:\n",
    "            order = np.argsort(dlist)\n",
    "            dlist = [dlist[i] for i in order]\n",
    "        idx_map = {d: i for i, d in enumerate(dlist)}\n",
    "        out[uid] = (dlist, idx_map)\n",
    "    return out\n",
    "\n",
    "DATE_INDEX = build_date_index(dates_map)\n",
    "\n",
    "def date_to_cutoff(uid, ds):\n",
    "    ds = pd.Timestamp(ds)\n",
    "    if uid not in DATE_INDEX:\n",
    "        return None\n",
    "    _, im = DATE_INDEX[uid]\n",
    "    return im.get(ds, None)\n",
    "\n",
    "station_mean_std = {\n",
    "    k: (float(np.mean(v)), float(np.std(v) + 1e-7))\n",
    "    for k, v in series_map.items()\n",
    "}\n",
    "\n",
    "def denormalize_predictions(pred, mean_std):\n",
    "    mean, std = mean_std\n",
    "    return pred * std + mean\n",
    "\n",
    "# ======================================================================\n",
    "# Build candidate bank (history + future segments)\n",
    "# ======================================================================\n",
    "cand_full_segments = []\n",
    "cand_series = []\n",
    "cand_end_idx = []\n",
    "cand_start_idx = []\n",
    "\n",
    "t_build = time.time()\n",
    "for uid, values in series_map.items():\n",
    "    x = np.asarray(values, dtype=float)\n",
    "    if len(x) < H + P:\n",
    "        continue\n",
    "    dates_uid = [pd.Timestamp(d) for d in dates_map[uid]]\n",
    "    stride = H \n",
    "    for end in range(H, len(x) - P + 1, stride):\n",
    "        end_date = dates_uid[end - 1]\n",
    "        if end_date >= eval_start:\n",
    "            continue \n",
    "        start = end - H\n",
    "        hist_seg = x[start:end]\n",
    "        fut_seg  = x[end:end+P]\n",
    "        if np.isnan(hist_seg).all() or np.isnan(fut_seg).all():\n",
    "            continue\n",
    "        if np.isnan(hist_seg).any():\n",
    "            hist_seg = (pd.Series(hist_seg).fillna(method=\"ffill\").fillna(method=\"bfill\").values)\n",
    "        if np.isnan(fut_seg).any():\n",
    "            fut_seg = (pd.Series(fut_seg).fillna(method=\"ffill\").fillna(method=\"bfill\").values)\n",
    "        full_seg = np.concatenate([hist_seg, fut_seg])\n",
    "        cand_full_segments.append(full_seg.astype(np.float32))\n",
    "        cand_series.append(uid)\n",
    "        cand_end_idx.append(end)\n",
    "        cand_start_idx.append(start)\n",
    "\n",
    "cand_full_segments = (np.stack(cand_full_segments)\n",
    "                      if cand_full_segments else np.zeros((0, H+P), dtype=np.float32))\n",
    "\n",
    "cand_series    = np.array(cand_series)\n",
    "cand_end_idx   = np.array(cand_end_idx)\n",
    "cand_start_idx = np.array(cand_start_idx)\n",
    "\n",
    "print(f\"[BUILD] Candidate segments: {cand_full_segments.shape} built in {time.time() - t_build:.2f}s\")\n",
    "\n",
    "uid_to_cand_indices = {}\n",
    "for i, s in enumerate(cand_series):\n",
    "    uid_to_cand_indices.setdefault(s, []).append(i)\n",
    "\n",
    "# History matrix for MI scoring\n",
    "cand_hist_matrix = cand_full_segments[:, :H] if cand_full_segments.shape[0] else np.zeros((0, H), dtype=np.float32)\n",
    "\n",
    "# ======================================================================\n",
    "# Helpers\n",
    "# ======================================================================\n",
    "def window_overlap_ratio(a_start, a_end, b_start, b_end):\n",
    "    \"\"\"\n",
    "    Overlap length / window length. Windows are [start, end) in indices.\n",
    "    Assumes equal lengths (H), but formula works generally.\n",
    "    \"\"\"\n",
    "    inter = max(0, min(a_end, b_end) - max(a_start, b_start))\n",
    "    length = max(a_end - a_start, 1)\n",
    "    return inter / length\n",
    "\n",
    "# ======================================================================\n",
    "# MI-Inspired Retrieval\n",
    "# ======================================================================\n",
    "def mi_score_histories(query_hist: np.ndarray,\n",
    "                       candidate_hist_matrix: np.ndarray,\n",
    "                       n_neighbors: int = 3,\n",
    "                       var_eps: float = 1e-8) -> np.ndarray:\n",
    "    if candidate_hist_matrix.shape[0] == 0:\n",
    "        return np.zeros((0,), dtype=float)\n",
    "\n",
    "    X = candidate_hist_matrix.T \n",
    "    y = query_hist.astype(float)\n",
    "    var_mask = X.var(axis=0) > var_eps\n",
    "    if not var_mask.any():\n",
    "        return np.zeros((candidate_hist_matrix.shape[0],), dtype=float)\n",
    "    X_use = X[:, var_mask]\n",
    "    scores_sub = mutual_info_regression(X_use, y, discrete_features=False, n_neighbors=n_neighbors)\n",
    "    scores = np.zeros(X.shape[1], dtype=float)\n",
    "    scores[var_mask] = scores_sub\n",
    "    return scores\n",
    "\n",
    "def mi_retrieve_topn(query_series, query_uid, cutoff_idx, top_n, n_neighbors=3):\n",
    "    if top_n == 0:\n",
    "        return np.array([], dtype=int), np.array([], dtype=float), query_series[cutoff_idx - H:cutoff_idx]\n",
    "    if cutoff_idx is None or cutoff_idx < H or cutoff_idx + P > len(query_series):\n",
    "        return None\n",
    "    ctx = query_series[cutoff_idx - H:cutoff_idx]\n",
    "    if len(ctx) != H or np.isnan(ctx).all():\n",
    "        return None\n",
    "    if np.isnan(ctx).any():\n",
    "        ctx = (pd.Series(ctx).fillna(method=\"ffill\").fillna(method=\"bfill\")).values\n",
    "    if cand_hist_matrix.shape[0] == 0:\n",
    "        return np.array([], dtype=int), np.array([], dtype=float), ctx\n",
    "\n",
    "    scores = mi_score_histories(ctx, cand_hist_matrix)\n",
    "\n",
    "    if query_uid in uid_to_cand_indices:\n",
    "        for idx in uid_to_cand_indices[query_uid]:\n",
    "            if cand_end_idx[idx] >= (cutoff_idx - 1):\n",
    "                scores[idx] = -np.inf\n",
    "\n",
    "    valid = ~np.isneginf(scores)\n",
    "    if not valid.any():\n",
    "        return np.array([], dtype=int), np.array([], dtype=float), ctx\n",
    "\n",
    "    vidx = np.where(valid)[0]\n",
    "    vscores = scores[valid]\n",
    "\n",
    "    # sort by MI desc\n",
    "    order = np.argsort(vscores)[::-1]\n",
    "\n",
    "    chosen_list = []\n",
    "    score_list  = []\n",
    "\n",
    "    for pos in order:\n",
    "        if len(chosen_list) >= top_n:\n",
    "            break\n",
    "        cand_i = int(vidx[pos])\n",
    "        keep = True\n",
    "        for taken in chosen_list:\n",
    "            if cand_series[cand_i] == cand_series[taken]:\n",
    "                ov = window_overlap_ratio(\n",
    "                    cand_start_idx[cand_i], cand_end_idx[cand_i],\n",
    "                    cand_start_idx[taken],   cand_end_idx[taken]\n",
    "                )\n",
    "                if ov >= OVERLAP_THRESH:\n",
    "                    keep = False\n",
    "                    break\n",
    "        if keep:\n",
    "            chosen_list.append(cand_i)\n",
    "            score_list.append(float(vscores[pos]))\n",
    "\n",
    "    return np.array(chosen_list, dtype=int), np.array(score_list, dtype=float), ctx\n",
    "\n",
    "# ======================================================================\n",
    "# Augmentation\n",
    "# ======================================================================\n",
    "def build_augmented_variants(query_ctx, top_indices):\n",
    "    ctx_t = torch.tensor(query_ctx, dtype=torch.float32)\n",
    "    m_c = float(ctx_t.mean())\n",
    "    s_c = float(ctx_t.std(unbiased=False) + 1e-7)\n",
    "    ctx_n = (ctx_t - m_c) / s_c\n",
    "\n",
    "    if len(top_indices) == 0:\n",
    "        return [ctx_n], [(m_c, s_c)], True\n",
    "    \n",
    "    variants = []\n",
    "    stats = []\n",
    "    zero_prefix = torch.zeros(H + P, dtype=torch.float32)\n",
    "    anchor_variant = torch.cat([zero_prefix, ctx_n], dim=0)\n",
    "    variants.append(anchor_variant)\n",
    "    stats.append((m_c, s_c))\n",
    "\n",
    "    for ci in top_indices:\n",
    "        seg_full = cand_full_segments[ci]\n",
    "        seg_t = torch.tensor(seg_full, dtype=torch.float32)\n",
    "        mask_s = ~torch.isnan(seg_t)\n",
    "        if mask_s.any():\n",
    "            m_s = float(seg_t[mask_s].mean())\n",
    "            s_s = float(seg_t[mask_s].std(unbiased=False) + 1e-7)\n",
    "        else:\n",
    "            m_s, s_s = 0.0, 1.0\n",
    "        seg_n = (seg_t - m_s) / s_s\n",
    "        shift = ctx_n[0] - seg_n[H - 1]\n",
    "        seg_n = seg_n + shift\n",
    "        combined = torch.cat([seg_n, ctx_n], dim=0)\n",
    "        variants.append(combined)\n",
    "        stats.append((m_c, s_c))\n",
    "\n",
    "    for v in variants:\n",
    "        assert v.shape[0] == AUG_LEN, f\"Variant length mismatch {v.shape[0]} != {AUG_LEN}\"\n",
    "    return variants, stats, False\n",
    "\n",
    "# ======================================================================\n",
    "# Forecast loop\n",
    "# ======================================================================\n",
    "retrieval_audit = []\n",
    "records = []\n",
    "eval_dates = pd.date_range(eval_start, eval_end, freq=\"D\")\n",
    "\n",
    "t_loop = time.time()\n",
    "for ds in tqdm(eval_dates, desc=\"MI Style Retrieval\"):\n",
    "    for uid in eval_uids:\n",
    "        cutoff = date_to_cutoff(uid, ds)\n",
    "        if cutoff is None or cutoff < min_history_points or cutoff + P > len(series_map[uid]):\n",
    "            continue\n",
    "        series_vals = np.asarray(series_map[uid], dtype=float)\n",
    "\n",
    "        ret = mi_retrieve_topn(series_vals, uid, cutoff, top_n)\n",
    "        if ret is None:\n",
    "            continue\n",
    "        top_idx, top_scores, query_ctx = ret\n",
    "\n",
    "        augmented_variants, stats, baseline_mode = build_augmented_variants(query_ctx, top_idx)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            samples = chronos.predict(\n",
    "                augmented_variants,\n",
    "                prediction_length=P,\n",
    "                num_samples=chronos.faux_num_samples\n",
    "            )\n",
    "        fc_matrix = samples.mean(dim=1).cpu().numpy()\n",
    "        K = fc_matrix.shape[0]\n",
    "\n",
    "        fc_matrix_denorm = np.zeros_like(fc_matrix)\n",
    "        for i in range(K):\n",
    "            m_c, s_c = stats[i]\n",
    "            fc_matrix_denorm[i] = fc_matrix[i] * s_c + m_c\n",
    "            \n",
    "        if baseline_mode:\n",
    "            final_fc = fc_matrix_denorm[0]\n",
    "            progressive = [fc_matrix_denorm[0]]\n",
    "        else:\n",
    "            if aggregate_mode == \"mean\":\n",
    "                final_fc = fc_matrix_denorm.mean(axis=0)\n",
    "                progressive = [fc_matrix_denorm[:i].mean(axis=0) for i in range(1, K + 1)]\n",
    "            elif aggregate_mode == \"dist_weighted\":\n",
    "                retrieved_scores = top_scores\n",
    "                dists = (retrieved_scores.max() - retrieved_scores) + 1e-8\n",
    "                anchor_proxy = np.median(dists) if dists.size else 1.0\n",
    "                d_all = np.concatenate([[anchor_proxy], dists])\n",
    "                w_all = np.exp(-alpha * d_all); w_all /= w_all.sum()\n",
    "                final_fc = (fc_matrix_denorm * w_all[:, None]).sum(axis=0)\n",
    "                progressive = []\n",
    "                for m in range(1, K + 1):\n",
    "                    d_part = d_all[:m]\n",
    "                    w_part = np.exp(-alpha * d_part); w_part /= w_part.sum()\n",
    "                    progressive.append((fc_matrix_denorm[:m] * w_part[:, None]).sum(axis=0))\n",
    "            elif aggregate_mode == \"mi_weighted\":\n",
    "                anchor_score = np.median(top_scores) if top_scores.size else 1.0\n",
    "                all_scores = np.concatenate([[anchor_score], top_scores])\n",
    "                w_all = all_scores / all_scores.sum()\n",
    "                final_fc = (fc_matrix_denorm * w_all[:, None]).sum(axis=0)\n",
    "                progressive = []\n",
    "                for m in range(1, K + 1):\n",
    "                    w_part = all_scores[:m]\n",
    "                    w_part = w_part / w_part.sum()\n",
    "                    progressive.append((fc_matrix_denorm[:m] * w_part[:, None]).sum(axis=0))\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported aggregate_mode\")\n",
    "\n",
    "        truth_future = series_vals[cutoff:cutoff + P]\n",
    "        mae_anchor = float(np.mean(np.abs(fc_matrix_denorm[0] - truth_future)))\n",
    "        mae_final  = float(np.mean(np.abs(final_fc - truth_future)))\n",
    "\n",
    "        audit_rec = {\n",
    "            \"unique_id\": uid,\n",
    "            \"ds\": str(ds),\n",
    "            \"cutoff_index\": int(cutoff),\n",
    "            \"baseline_mode\": baseline_mode,\n",
    "            \"num_retrieved\": 0 if baseline_mode else int(len(top_idx)),\n",
    "            \"anchor_future_mae\": mae_anchor,\n",
    "            \"ensemble_future_mae\": mae_final,\n",
    "            \"aggregation\": aggregate_mode,\n",
    "            \"retrieved\": [],\n",
    "            \"progressive_mae\": [\n",
    "                {\n",
    "                    \"num_contexts\": i,\n",
    "                    \"mae\": float(np.mean(np.abs(progressive[i-1] - truth_future)))\n",
    "                } for i in range(1, len(progressive) + 1)\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        if not baseline_mode:\n",
    "            for rank, (ci, score) in enumerate(zip(top_idx, top_scores), 1):\n",
    "                raw_seg = cand_full_segments[ci]\n",
    "                fut_seg = raw_seg[H:H+P]\n",
    "                start_idx = cand_start_idx[ci]\n",
    "                end_idx   = cand_end_idx[ci]\n",
    "                audit_rec[\"retrieved\"].append({\n",
    "                    \"rank\": rank,\n",
    "                    \"mi_score\": float(score),\n",
    "                    \"source_uid\": cand_series[ci],\n",
    "                    \"candidate_index\": int(ci),\n",
    "                    \"window_start_index\": int(start_idx),\n",
    "                    \"window_end_index\": int(end_idx),\n",
    "                    \"window_start_date\": str(dates_map[cand_series[ci]][start_idx]),\n",
    "                    \"window_end_date\": str(dates_map[cand_series[ci]][end_idx - 1]),\n",
    "                    \"future_values\": fut_seg.tolist()\n",
    "                })\n",
    "\n",
    "        retrieval_audit.append(audit_rec)\n",
    "\n",
    "        # store rows\n",
    "        for h in range(P):\n",
    "            records.append({\n",
    "                \"unique_id\": uid,\n",
    "                \"ds\": ds,\n",
    "                \"h\": h + 1,\n",
    "                \"pred\": float(final_fc[h]),\n",
    "                \"truth\": float(truth_future[h])\n",
    "            })\n",
    "\n",
    "print(f\"[LOOP] Done in {time.time() - t_loop:.2f}s; audit entries: {len(retrieval_audit)}\")\n",
    "\n",
    "# Define output path for raw predictions CSV\n",
    "raw_preds_path = Path(f\"KSTUDY_raw_preds_top{top_n}_{prediction_length}days_MI.csv\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# METRICS  (overall, per-station) and write raw preds CSV\n",
    "# ----------------------------------------------------------------------\n",
    "df_aug = pd.DataFrame(records)\n",
    "if df_aug.empty:\n",
    "    raise RuntimeError(\"No rows produced. Adjust config/date ranges/data availability.\")\n",
    "\n",
    "# Write raw preds & truth to a single CSV\n",
    "df_aug.to_csv(raw_preds_path, index=False)\n",
    "logger.info(f\"Wrote raw predictions to {raw_preds_path}\")\n",
    "\n",
    "# Overall metrics across all points\n",
    "err_all     = df_aug.pred - df_aug.truth\n",
    "overall_mae = float(np.mean(np.abs(err_all)))\n",
    "overall_rmse= float(np.sqrt(np.mean(err_all**2)))\n",
    "\n",
    "print(\"=== MI Style Retrieval ===\")\n",
    "\n",
    "# Per-station metrics\n",
    "def station_metrics(group):\n",
    "    err  = group.pred - group.truth\n",
    "    mae  = float(np.mean(np.abs(err)))\n",
    "    rmse = float(np.sqrt(np.mean(err**2)))\n",
    "    return pd.Series({\"MAE\": mae, \"RMSE\": rmse})\n",
    "\n",
    "per_station = (\n",
    "    df_aug\n",
    "      .groupby(\"unique_id\")\n",
    "      .apply(station_metrics)\n",
    "      .reset_index()\n",
    "      .rename(columns={\"unique_id\": \"station\"})\n",
    ")\n",
    "\n",
    "print(\"Per-station metrics:\")\n",
    "print(per_station.to_string(index=False))\n",
    "\n",
    "# Average across stations\n",
    "avg_mae  = per_station.MAE.mean()\n",
    "avg_rmse = per_station.RMSE.mean()\n",
    "print(f\"\\nAVERAGE ACROSS STATIONS: MAE={avg_mae:.6f} | RMSE={avg_rmse:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
